---
title: "Scratch Paper"
author: "Colin Busby"
date: '`r Sys.Date()`'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
# install.packages("rstatix")
```


```{r}
library(tidyverse)
library(randomForest)
library(parallel)
library(rpart)
library(readr)
library(janitor)
library(car)
library(MASS)
library(Hmisc)
library(rstatix)
```

## Tidyverse Experimenting

```{r}
my_data <- as_tibble(ap_clean_f)
class(my_data)
```

```{r}
my_data
```

### Random Forest Confusion Matrix - DON'T USE, NOT WORKING; Mutate() and if_else() good examples

`Salary` is not a binary response, so table is too large ("[ reached getOption("max.print") -- omitted 296 rows ]").

Possible way is to do what Alex and Winnie did, with cut-off for data i.e. Salary > 0.7 as 0,1.  Will probably need to ask about this.

Not working

```{r}
# my_data$fSalary %>% mutate(my_data, fSalary=if_else(Salary >= quantile(Salary, probs = 0.75), 1, 0))
# #set.seed(999)
# rf4 <- randomForest(Salary ~ fPosition + Age + Weight, data=my_data, n.var = 10, importance=TRUE)
# rf_preds_4 <- predict(rf4, type = "response")
# varImpPlot(rf4, type=1, main=" ")
```

```{r}
# tb <- table(actual = my_data$fSalary, predicted = rf_preds_4)
# addmargins(tb)
```

## Forest - Numerical Data

```{r}
# Split into Train and Validation sets
# Training Set : Validation Set = 70 : 30 (random)
set.seed(100)
train <- sample(nrow(ap_clean_f), 0.7*nrow(ap_clean_f), replace = FALSE)
TrainSet <- ap_clean_f[train,]
ValidSet <- ap_clean_f[-train,]
summary(TrainSet)
summary(ValidSet)
```

```{r}
# Create a Random Forest model with default parameters
model1 <- randomForest(Salary ~ . - College - Team - Name - fCollege - fTeam, data = TrainSet, importance = TRUE)
model1
```

```{r}
# WIP

#ap_clean_college <- active_p[!(is.na(active_p$Salary)), ]
#ap_cf_wo <- subset(ap_clean_f, !score %in% identify_outliers(ap_clean_f, "score")$score)
```


## Removal?

```{r}
# Removal of `fTeam` due to no significance
summary(lm2 <- lm(Salary ~ fPosition+fCollege+poly(Age,3)+Height_i+Weight,
                  data=ap_tibble))
```

```{r}
# Removal of `Height_i` due to no significance
summary(lm3 <- lm(Salary ~ fPosition+fCollege+poly(Age,3)+Weight,
                  data=ap_tibble))
```

```{r}
# Removal of `fCollege`
summary(lm4 <- lm(Salary ~ fPosition+poly(Age,3)+Weight,
                  data=ap_tibble))
```

```{r}
lm1 <- lm(Salary ~ fTeam+fPosition+fCollege+poly(Age,3)+Height_i+Weight,
          data=ap_tibble)
```

```{r}
anova(lm4,lm3,lm2,lm1)
```


## RF

```{r}
# rf_model <- caret::train(log(Salary) ~ fTeam+fPosition+fCollege+poly(Age,3)+Height_i+Weight,
#                          data = ap_tibble,
#                          method = "rf")
# 
# rf_model
```

### Random Forest

```{r}
set.seed(999)
rf3 <- randomForest(log(Salary) ~ fPosition+poly(Age,3)+Height_i+Weight,data = ap_tibble, n.var = 10, importance=TRUE)
rf_preds_3 <- predict(rf3, type = "response")
varImpPlot(rf3, type=1, main=" ")
```

## Logistic/Linear?

```{r}
#plot(Salary ~ fPosition+Age+Weight+fPosition:Age+fPosition:Weight+Age:Weight,data=ap_tibble)


#pairs(Salary ~ fTeam+fPosition+fCollege+Age+Height_i+Weight,data=ap_tibble)

#pairs(Salary ~ fPosition+Age+Weight+fPosition:Age+fPosition:Weight+Age:Weight,data=ap_tibble)

# lmst <- step(lm1)
# summary(lmst)
# 
# plot(lm1, which = 1)
# plot(lm1, which = 2)
# 
# plot(lm4, which = 1)
# plot(lm4, which = 2)


head(ap_tibble)

#check 


boxcox(lm1, lambda = seq(-2,2, by=0.05))
summary(powerTransform(lm1))

summary(lm1_log <- lm(log(Salary) ~ fTeam+fPosition+fCollege+Age+Height_i+Weight,
                      data=ap_tibble))

AIC(lm1, lm4, lmst)
```


## Cross-validation

```{r}
# cross validation
set.seed(999)
n <-nrow(ap_tibble)
n
floor(0.7*n)

train <- sample(1:n, 266, replace = FALSE)

tn <- ap_tibble[train,]
test <- ap_tibble[-train,]
```

```{r}
# Backwards Stepwise

summary(lm_test.step <- lm(log(Salary) ~ fPosition+poly(Age,3)+Weight,data=tn))

predictions2 <- lm_test.step %>% predict(test)
data.frame(R2 = R2(predictions, test$Salary),
RMSE = RMSE(predictions, test$Salary),
MAE = MAE(predictions, test$Salary))
```

```{r}
# Backwards Elimination

lm_test <- lm(log(Salary) ~ fPosition+fCollege+poly(Age,3)+Weight,data=tn)

lm_test$xlevels[["fCollege"]] <- union(lm_test$xlevels[["fCollege"]], levels(test$fCollege))

predictions <- lm_test %>% predict(test)
data.frame(R2 = R2(predictions, test$Salary),RMSE = RMSE(predictions, test$Salary),MAE = MAE(predictions, test$Salary))
```