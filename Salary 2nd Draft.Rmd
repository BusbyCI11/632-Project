---
title: "Salary MLR 2nd Draft"
author: "Colin Busby"
date: "`r Sys.Date()`"
output:
  pdf_document: 
    toc: yes
    toc_depth: 5
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(tidyverse)
library(randomForest)
library(parallel)
library(rpart)
library(readr)
library(janitor)
library(car)
library(MASS)
library(Hmisc)
library(rstatix)
library(forcats)
library(GGally)
library(performance)
library(caret)
library(lmtest)
library(corrplot)
library(PerformanceAnalytics)
library(plotly)
library(mlbench)
```

# Intro

blahblahblah Hi

Question - How do the various stats from the ESPN website explain the salaries of NBA players?

Where data is from

Variables explained

## Data Exploration

```{r}
# Load dataset

players <- read_csv("players.csv")
active_p <- read_csv("active_players_2.csv")
```

```{r}
# Structure/Dimensions of the data.frame
str(active_p)
```

```{r}
# Number of columns containing `NA` or `NaN`
colSums(is.na(active_p))
```

## Data Cleaning

```{r}
#identifying the rows with NAs
rownames(active_p)[apply(active_p, 2, anyNA)]

#removing all `Salary` observations with NAs
ap_cl <- active_p[!(is.na(active_p$Salary)), ]
```

```{r}
# Checking for leftover `NA` after cleaning
rownames(active_p)[apply(ap_cl, 2, anyNA)]
```

```{r}
# Transforming Team,Position,College into factors
ap_cl_f <- transform (
  ap_cl,
  fTeam = as.factor(Team),
  fPosition = as.factor(Position),
  fCollege = as.factor(College)
)
```

```{r}
# Rename `fCollege` factor level "nan" as "None"
# levels(ap_cl_f$fCollege)[levels(ap_cl_f$fCollege)=="nan"] <- "None"
#levels(ap_cl_f$fCollege)
```

```{r}
str(ap_cl_f)
```

## Recoding for Logistic Regression

```{r}
# Start with `ap_cl_f` data.frame, reclassify as tibble

ap_tibble <- as_tibble(ap_cl_f)
class(ap_tibble)
```

```{r}
# Calculate quantiles of Salary for good cutoff point
quantile(ap_tibble$Salary, probs = seq(0,1,1/20))
```

```{r}
ap_tibble <- ap_tibble %>% mutate(Salary.Dummy = if_else(Salary >= 8222063, 1, 0))
```

```{r}
str(ap_tibble)
```

# Model Building

### Initial Graphs

```{r}
# The diagonal consists of the densities of the three variables and the upper panels consist of the correlation coefficients between the variables.

ggpairs(ap_tibble, columns = c(9,4,6,7), cardinality_threshold = NULL)
```

```{r}
ggplot(ap_tibble, aes(Salary)) +
  geom_histogram(bins = 30)
```

```{r}
ggplot(ap_tibble, aes(log(Salary))) +
  geom_histogram(bins = 30)
```

### Full(Initial) Model

```{r}
summary(lm1 <- lm(Salary ~ fTeam+fPosition+fCollege+Age+Height_i+Weight, data=ap_tibble))
```

```{r}
shapiro.test(resid(lm1))
bptest(lm1)
```

### log(Full Model)

```{r}
summary(lm_l1 <- lm(log(Salary) ~ fTeam+fPosition+fCollege+Age+Height_i+Weight, data=ap_tibble))
```

```{r}
shapiro.test(resid(lm_l1))
bptest(lm_l1)
```

### Check for Polynomial Terms

`Age`, `Height_i`, and `Weight` appear to be misspecified.

```{r}
crPlots(lm_l1)
```

```{r}
#summary(lm(Salary ~ fTeam+fPosition+fCollege+poly(Age,2)+poly(Height_i,2)+poly(Weight,2),data=ap_tibble))
#summary(lm(Salary ~ fTeam+fPosition+fCollege+poly(Age,3)+poly(Height_i,3)+poly(Weight,3),data=ap_tibble))
#summary(lm(Salary ~ fTeam+fPosition+fCollege+poly(Age,4)+poly(Height_i,4)+poly(Weight,4),data=ap_tibble))
#summary(lm(Salary ~ fTeam+fPosition+fCollege+poly(Age,3)+Height_i+poly(Weight,2),data=ap_tibble))
#summary(lm(Salary ~ fTeam+fPosition+fCollege+poly(Age,3)+Height_i+poly(Weight,3),data=ap_tibble))
#summary(lm(Salary ~ fTeam+fPosition+fCollege+poly(Age,3)+Height_i+poly(Weight,4),data=ap_tibble))
summary(lm(Salary ~ fTeam+fPosition+fCollege+poly(Age,3)+Height_i+Weight,data=ap_tibble))
```

The end result is that only `Age` has been misspecified, and has been re-specified as `poly(Age,3)`.

### Reduced Model

```{r}
summary(lm_log1 <- lm(log(Salary) ~ fTeam+fPosition+fCollege+poly(Age,3)+Height_i+Weight, data=ap_tibble))
```


```{r}
# Removal of `fTeam` due to no significance
summary(lm_log2 <- lm(log(Salary) ~ fPosition+fCollege+poly(Age,3)+Height_i+Weight,data=ap_tibble))
```

```{r}
summary(lm_log3 <- lm(log(Salary) ~ fPosition+poly(Age,3)+Height_i+Weight,data=ap_tibble))
```

```{r}
summary(lm_log4 <- lm(log(Salary) ~ fPosition+poly(Age,3)+Weight,data=ap_tibble))
```

```{r}
anova(lm_log4,lm_log3,lm_log2,lm_log1)
```

### Stepwise Model Selection

```{r}
summary(lm_step <- step(lm_logsm <- lm(log(Salary) ~ fTeam+fPosition+fCollege+poly(Age,3)+Height_i+Weight,data=ap_tibble)))
```

```{r}
shapiro.test(resid(lm_log2))
bptest(lm_log2)
```

```{r}
shapiro.test(resid(lm_step))
bptest(lm_step)
```

### Logistic Regression

```{r}
# Transforming Team,Position,College into factors
ap_tibble <- transform (
  ap_tibble,
  nTeam = as.numeric(fTeam),
  nPosition = as.numeric(fPosition),
  nCollege = as.numeric(fCollege),
  fSalary.Dummy = as.factor(Salary.Dummy)
)

#cross validation
set.seed(999)
n <-nrow(ap_tibble)
n
floor(0.7*n)

#randomly sample 70% of the rows

train <- sample(1:n, 311)

tn <- ap_tibble[train,]
test <- ap_tibble[-train,]
```

```{r}
summary(glmFit <- train(fSalary.Dummy ~ nTeam+nPosition+nCollege+Age+Height_i+Weight, data=ap_tibble, method="glmStepAIC", trace = 0))
confusionMatrix(glmFit)
```

```{r}
summary(glmFit2 <- train(fSalary.Dummy ~ nTeam+nPosition+nCollege+poly(Age,3)+Height_i+Weight, data=ap_tibble, method="glmStepAIC", trace = 0))
confusionMatrix(glmFit2)
```

```{r}
#comparing the plots
# Rename variables

plot(lm_log1, 1:2)

plot(lm_log4, 1:2)

hist(resid(lm_log1))
hist(resid(lm_log4))


par(mfrow=c(1,3), mar=c(4.5, 4.5, 2, 2))

plot(ap_tibble$fPosition, rstandard(lm_log1), xlab ="fposition", ylab = "residuals")
plot(ap_tibble$fPosition, rstandard(lm_log4), xlab ="fposition", ylab = "residuals")

plot(ap_tibble$Age, rstandard(lm_log1), xlab ="fposition", ylab = "residuals")
plot(ap_tibble$Age, rstandard(lm_log4), xlab ="fposition", ylab = "residuals")

plot(ap_tibble$Weight, rstandard(lm_log1), xlab ="fposition", ylab = "residuals")
plot(ap_tibble$Weight, rstandard(lm_log4), xlab ="fposition", ylab = "residuals")

#normality check 
par(mfrow=c(1,2), mar=c(4.5, 4.5, 2, 2))
plot(predict(lm_log1), ap_tibble$Salary,
       xlab = "Fitted Values", ylab = "Salary")
abline()
lines(lowess(predict(lm_log1), ap_tibble$Salary), col='red')

plot(predict(lm_log4), sqrt(ap_tibble$Salary),
       xlab = "Fitted Values", ylab = "Salary")
lines(lowess(predict(lm_log4), sqrt(ap_tibble$Salary)), col='red')

plot(predict(lm_log1), ap_tibble$Weight,
     xlab = "Fitted Values", ylab = "Weight")

plot(predict(lm_log4), sqrt(ap_tibble$Weight),
     xlab = "Fitted Values", ylab = "Weight")

plot(predict(lm_log1), ap_tibble$Age,
     xlab = "Fitted Values", ylab = "Age")

plot(predict(lm_log4), sqrt(ap_tibble$Age),
     xlab = "Fitted Values", ylab = "Age")
```


## Model Checking

### Outliers

```{r}
# Remove outliers & high leverage from data
ap_cl_f_wo <- ap_tibble[-which(abs(rstandard(lm_step)) > 2
                               | hatvalues(lm_step) > .1),]
```

```{r}
summary(lm1_wo <- lm(log(Salary) ~ fTeam+fPosition+fCollege+poly(Age,3)+Height_i+Weight,
                     data=ap_cl_f_wo))
summary(lm1_wo_step <- step(lm1_wo))
```

### Box Cox

```{r}
boxcox(lm1,lambda=seq(-1, 1, by=0.05))
```

```{r}
BoxCoxTrans(ap_tibble$Salary)
```

```{r}
# Performing
summary(lm_trans_step <- step(lm1_trans <- lm(log(Salary)~fTeam + fPosition + fCollege + poly(Age,3) + Height_i + Weight, data = ap_tibble)))
```

### Box-Cox w/o outliers and high leverage

Very poor results compared to keeping the high leverage points and the outliers.

```{r}
boxcox(lm1_wo,lambda=seq(-7, 7, by=0.05))
```

```{r}
BoxCoxTrans(ap_cl_f_wo$Salary)
```

```{r}
summary(lm_wo_step_trans <- step(lm1_wo_trans <- lm(log(Salary)~fTeam + fPosition + fCollege + poly(Age,3) + Height_i + Weight, data = ap_cl_f_wo)))
```

## Comparing Models

```{r}
performance::compare_performance(lm_log1,lm_log2,lm_log3,lm_step, rank = TRUE)
```

```{r}
anova(lm_trans_step,lm1_trans)
```

```{r}
compare_performance(lm1_wo,lm_wo_step_trans, rank = TRUE)
```



```{r}
par(mfrow=c(2,2))
plot(lm_step,1:2)
plot(lm1_wo_trans,1:2)
```

## VIF

```{r}
round(vif(lm_step),2)
round(vif(lm_trans_step),2)
```

## ROC/AUC
```{r}
set.seed(999)
n <-nrow(ap_tibble)
n
floor(0.7*n)

#randomly sample 70% of the rows

train <- sample(1:n, 311)

glm_train <- glm(fSalary.Dummy ~ poly(Age,3), data=ap_tibble, subset = train, family =  binomial)

summary(glm_train)

ap_test <- ap_tibble[-train,]
head(ap_test)

probs_test <- predict(glm_train, newdata = ap_test,
                     type = "response")

head(probs_test)


length(probs_test)
preds_test <- rep(0,12)
preds_test[probs_test > 0.5] <- 1

length(probs_test)
length(preds_test)

head(probs_test)
head(preds_test)

tb1<- table(prediction = preds_test, actual = ap_test$fSalary.Dummy)
addmargins(tb1)

#Accuracy
(tb1[1,1] + tb1[2,2]) / 1206
#Sensitivity
tb1 / 786
#Specificity
tb1[1,1] / 148


library(pROC)
roc_reduced <- roc(ap_test$fSalary.Dummy, probs_test)
plot(1 - roc_reduced$specificities, roc_reduced$sensitivities, type="l",
       xlab = "1 - Specificity", ylab = "Sensitivity")
# plot red point corresponding to 0.5 threshold:
points(x = 24/149, y = 763/785, col="red", pch=19)
abline(0, 1, lty=2)
```

```{r}
set.seed(999)
n <-nrow(ap_tibble)
n
floor(0.7*n)

#randomly sample 70% of the rows

train <- sample(1:n, 311)

glm_train <- glm(fSalary.Dummy ~ nTeam+nPosition+nCollege+poly(Age,3)+Height_i+Weight, data=ap_tibble, subset = train, family =  binomial)

summary(glm_train)

ap_test <- ap_tibble[-train,]
head(ap_test)

probs_test <- predict(glm_train, newdata = ap_test,
                     type = "response")

head(probs_test)


length(probs_test)
preds_test <- rep(0,12)
preds_test[probs_test > 0.5] <- 1

length(probs_test)
length(preds_test)

head(probs_test)
head(preds_test)

tb2<- table(prediction = preds_test, actual = ap_test$fSalary.Dummy)
addmargins(tb2)

#Accuracy
(tb2[1,1] + tb2[2,2]) / 1206
#Sensitivity
tb2[2,2] / 786
#Specificity
tb2[1,1] / 148


library(pROC)
roc_full <- roc(ap_test$fSalary.Dummy, probs_test)
plot(1 - roc_full$specificities, roc_full$sensitivities, type="l",
       xlab = "1 - Specificity", ylab = "Sensitivity")
# plot red point corresponding to 0.5 threshold:
points(x = 24/149, y = 763/785, col="red", pch=19)
abline(0, 1, lty=2)
```

```{r}
auc(roc_full)
```

```{r}
auc(roc_reduced)
```


## Plots

```{r}
plot(lm_log2, 1:2)
```

```{r}
check_model(lm_log4)
```


```{r}
plot(hatvalues(lm_step), rstandard(lm_trans_step),xlab='Leverage', ylab='Standardized Residuals')
```

```{r}
par(mfrow=c(1,2), mar=c(4.5, 4.5, 2, 2))
plot(lm_step, 1:2)
```




```{r}
performance::check_model(lm_step)
```

## Final Model - Change

$log(\widehat{Salary})=13.330870-0.489098(fPositionF)+0.73758(fPositionG)\\+0.174562(fPositionPF)+0.711135(fPositionPG)\\+0.316997(fPositionSF)+0.403881(fPositionSG)\\+6.769471(Age)-3.110142(Age)^2-3.153257(Age)^3+0.008296(Weight)$

### Prediction - Needs to be updated for newest model

```{r}
# Steph Curry, Age = 34, Weight = 185, fPosition = PG, Salary = 45780966
newdata <- data.frame(fPosition='PG', Age=34, Weight = 185)
exp(predict(lm_trans_step, newdata, type='response'))
```

```{r}
# Miles McBride Age = 20, Weight = 200, fPosition = PG, Salary = 925258
newdata <- data.frame(fPosition='PG', Age=20, Weight = 200)
exp(predict(lm_trans_step, newdata, type='response'))
```

```{r}
newdata <- data.frame(fPosition=ap_tibble$fPosition, Age=ap_tibble$Age, Weight = ap_tibble$Weight)
exp(predict(lm_trans_step, newdata, type='response'))
```

## Conclusion

Given that the estimated/predicted salaries are way way off, and that the adjusted $R^2$ of the `lm_step_trans` is only $0.171$, there are obviously other predictors responsible for the majority of variance in the salaries of NBA players, most likely factors such as points scored, assists, rebounds, etc.

It was never expected that non-skill factors would be responsible for explaining the majority of Salary variance.
